{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "For a gentle introduction see PyTorch extension tutorial.\n",
    "\n",
    "Source for torch.autograd.Function available here. These are the two that we have to override:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "@staticmethod\n",
    "def forward(ctx, *args, **kwargs):\n",
    "    \"\"\"Performs the operation.\n",
    "    This function is to be overridden by all subclasses.\n",
    "    It must accept a context ctx as the first argument, followed by any\n",
    "    number of arguments (tensors or other types).\n",
    "    The context can be used to store tensors that can be then retrieved\n",
    "    during the backward pass.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "@staticmethod\n",
    "def backward(ctx, *grad_outputs):\n",
    "    \"\"\"Defines a formula for differentiating the operation.\n",
    "    This function is to be overridden by all subclasses.\n",
    "    It must accept a context :attr:`ctx` as the first argument, followed by\n",
    "    as many outputs did :func:`forward` return, and it should return as many\n",
    "    tensors, as there were inputs to :func:`forward`. Each argument is the\n",
    "    gradient w.r.t the given output, and each returned value should be the\n",
    "    gradient w.r.t. the corresponding input.\n",
    "    The context can be used to retrieve tensors saved during the forward\n",
    "    pass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple\n",
    "    of booleans representing whether each input needs gradient. E.g.,\n",
    "    :func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the\n",
    "    first input to :func:`forward` needs gradient computated w.r.t. the\n",
    "    output.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom addition module\n",
    "class MyAdd(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x1, x2):\n",
    "        # ctx is a context where we can save computations for backward\n",
    "        ctx.save_for_backward(x1, x2)\n",
    "        return x1 + x2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x1, x2 = ctx.saved_tensors\n",
    "        grad_x1 = grad_output * torch.ones_like(x1)\n",
    "        grad_x2 = grad_output * torch.ones_like(x2)\n",
    "\n",
    "        # need to return grads in order of inputs to forward (excluding ctx)\n",
    "        return grad_x1, grad_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x1: tensor([-1.0880,  0.6012,  0.5684], requires_grad=True)\nx2: tensor([-0.5249, -0.9479, -0.1870], requires_grad=True)\ny: tensor([-1.6129, -0.3467,  0.3814], grad_fn=<MyAddBackward>)\nz: -0.5260655879974365, z.grad_fn: <MeanBackward0 object at 0x7fc2b96164a8>\nx1.grad: tensor([0.3333, 0.3333, 0.3333])\nx2.grad: tensor([0.3333, 0.3333, 0.3333])\n"
     ]
    }
   ],
   "source": [
    "# Trying out the custom addition module\n",
    "x1 = torch.randn((3), requires_grad= True)\n",
    "x2 = torch.randn((3), requires_grad= True)\n",
    "print(f'x1: {x1}')\n",
    "print(f'x2: {x2}')\n",
    "myadd = MyAdd.apply # aliasing the apply method\n",
    "y = myadd(x1, x2)\n",
    "print(f'y: {y}')\n",
    "z = y.mean()\n",
    "print(f'z: {z}, z.grad_fn: {z.grad_fn}')\n",
    "z.backward()\n",
    "print(f'x1.grad: {x1.grad}')\n",
    "print(f'x2.grad: {x2.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom split module\n",
    "class MySplit(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        x1 = x.clone()\n",
    "        x2 = x.clone()\n",
    "        return x1, x2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_x1, grad_x2):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        print(f'grad_x1: {grad_x1}')\n",
    "        print(f'grad_x2: {grad_x2}')\n",
    "        return grad_x1 + grad_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x: tensor([-1.3820,  0.1714,  1.3277, -1.2228], requires_grad=True)\nx1: tensor([-1.3820,  0.1714,  1.3277, -1.2228], grad_fn=<MySplitBackward>)\nx2: tensor([-1.3820,  0.1714,  1.3277, -1.2228], grad_fn=<MySplitBackward>)\ny: tensor([-2.7640,  0.3427,  2.6554, -2.4457], grad_fn=<AddBackward0>)\nz: -0.5528673529624939, z.grad_fn: <MeanBackward0 object at 0x7fc2ccdc9ac8>\ngrad_x1: tensor([0.2500, 0.2500, 0.2500, 0.2500])\ngrad_x2: tensor([0.2500, 0.2500, 0.2500, 0.2500])\nx.grad: tensor([0.5000, 0.5000, 0.5000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "# Trying out split module\n",
    "x = torch.randn((4), requires_grad = True)\n",
    "print(f'x: {x}')\n",
    "split = MySplit.apply\n",
    "x1, x2 = split(x)\n",
    "print(f'x1: {x1}')\n",
    "print(f'x2: {x2}')\n",
    "y = x1 + x2\n",
    "print(f'y: {y}')\n",
    "z = y.mean()\n",
    "print(f'z: {z}, z.grad_fn: {z.grad_fn}')\n",
    "z.backward()\n",
    "print(f'x.grad: {x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Max Module\n",
    "class MyMax(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        # where we explicitly use non-torch code\n",
    "        maximum = x.detach().numpy().max()\n",
    "        argmax = x.detach().eq(maximum).float()\n",
    "        ctx.save_for_backward(argmax)\n",
    "        return torch.tensor(maximum)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        argmax = ctx.saved_tensors[0]\n",
    "        return grad_output * argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x: tensor([0.1180, 1.0197, 2.2509, 0.8385, 1.7778], requires_grad=True)\ny: 2.250889539718628, y.grad_fn: <torch.autograd.function.MyMaxBackward object at 0x7fc3420f6ec8>\nx.grad: tensor([0., 0., 1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Trying out the max module\n",
    "x = torch.randn((5), requires_grad = True)\n",
    "print(f'x: {x}')\n",
    "mymax = MyMax.apply\n",
    "y = mymax(x)\n",
    "print(f'y: {y}, y.grad_fn: {y.grad_fn}')\n",
    "y.backward()\n",
    "print(f'x.grad: {x.grad}')"
   ]
  }
 ]
}