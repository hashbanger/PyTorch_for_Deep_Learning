{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37064bitpytorchdlconda5a5314c7707b4a7f9947b8a74663578b",
   "display_name": "Python 3.7.0 64-bit ('pytorchdl': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Autograd: Automatic Differentiation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The autograd package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 2.],\n        [3., 4.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Creating a 2x2 tensor with gradient-accumulation capabilities\n",
    "x = torch.tensor([[1, 2], [3, 4]], requires_grad = True, dtype = torch.float32)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-1.,  0.],\n        [ 1.,  2.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# performing an operation on the tensor\n",
    "# deducting 2 from all the elements\n",
    "y = x - 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<SubBackward0 object at 0x7f3c15e96c88>\n"
     ]
    }
   ],
   "source": [
    "# y is creates as a result of an operation, so it has a grad_fn\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<SubBackward0 at 0x7f3c15e96908>"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AccumulateGrad at 0x7f3c02be3cc0>"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "y.grad_fn.next_functions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "y.grad_fn.next_functions[0][0].variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 3.,  0.],\n        [ 3., 12.]], grad_fn=<MulBackward0>)\ntensor(4.5000, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# do more operations on y\n",
    "z = y * y * 3\n",
    "a = z.mean() # average\n",
    "\n",
    "print(z)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the computational graph\n",
    "from torchviz import make_dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f3c02be3e48>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"109pt\" height=\"381pt\"\n viewBox=\"0.00 0.00 109.00 381.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 377)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-377 105,-377 105,4 -4,4\"/>\n<!-- 139896044443472 -->\n<g id=\"node1\" class=\"node\">\n<title>139896044443472</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"77.5,-31 23.5,-31 23.5,0 77.5,0 77.5,-31\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 139895721206280 -->\n<g id=\"node2\" class=\"node\">\n<title>139895721206280</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"98,-86 3,-86 3,-67 98,-67 98,-86\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MeanBackward0</text>\n</g>\n<!-- 139895721206280&#45;&gt;139896044443472 -->\n<g id=\"edge7\" class=\"edge\">\n<title>139895721206280&#45;&gt;139896044443472</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.5,-66.79C50.5,-60.07 50.5,-50.4 50.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54,-41.19 50.5,-31.19 47,-41.19 54,-41.19\"/>\n</g>\n<!-- 139895720785232 -->\n<g id=\"node3\" class=\"node\">\n<title>139895720785232</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"95,-141 6,-141 6,-122 95,-122 95,-141\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 139895720785232&#45;&gt;139895721206280 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139895720785232&#45;&gt;139895721206280</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.5,-121.75C50.5,-114.8 50.5,-104.85 50.5,-96.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54,-96.09 50.5,-86.09 47,-96.09 54,-96.09\"/>\n</g>\n<!-- 139895720787080 -->\n<g id=\"node4\" class=\"node\">\n<title>139895720787080</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"95,-196 6,-196 6,-177 95,-177 95,-196\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 139895720787080&#45;&gt;139895720785232 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139895720787080&#45;&gt;139895720785232</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.5,-176.75C50.5,-169.8 50.5,-159.85 50.5,-151.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54,-151.09 50.5,-141.09 47,-151.09 54,-151.09\"/>\n</g>\n<!-- 139896042383624 -->\n<g id=\"node5\" class=\"node\">\n<title>139896042383624</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"95,-251 6,-251 6,-232 95,-232 95,-251\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">SubBackward0</text>\n</g>\n<!-- 139896042383624&#45;&gt;139895720787080 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139896042383624&#45;&gt;139895720787080</title>\n<path fill=\"none\" stroke=\"black\" d=\"M45.33,-231.75C43.84,-224.8 43.4,-214.85 44.02,-206.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"47.51,-206.47 45.37,-196.09 40.57,-205.53 47.51,-206.47\"/>\n</g>\n<!-- 139896042383624&#45;&gt;139895720787080 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139896042383624&#45;&gt;139895720787080</title>\n<path fill=\"none\" stroke=\"black\" d=\"M55.67,-231.75C57.16,-224.8 57.6,-214.85 56.98,-206.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"60.43,-205.53 55.63,-196.09 53.49,-206.47 60.43,-205.53\"/>\n</g>\n<!-- 139895720787136 -->\n<g id=\"node6\" class=\"node\">\n<title>139895720787136</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-306 0,-306 0,-287 101,-287 101,-306\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 139895720787136&#45;&gt;139896042383624 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139895720787136&#45;&gt;139896042383624</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.5,-286.75C50.5,-279.8 50.5,-269.85 50.5,-261.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54,-261.09 50.5,-251.09 47,-261.09 54,-261.09\"/>\n</g>\n<!-- 139896044442104 -->\n<g id=\"node7\" class=\"node\">\n<title>139896044442104</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"80,-373 21,-373 21,-342 80,-342 80,-373\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\"> (2, 2)</text>\n</g>\n<!-- 139896044442104&#45;&gt;139895720787136 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139896044442104&#45;&gt;139895720787136</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.5,-341.92C50.5,-334.22 50.5,-324.69 50.5,-316.43\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54,-316.25 50.5,-306.25 47,-316.25 54,-316.25\"/>\n</g>\n</g>\n</svg>\n"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "make_dot(a)"
   ]
  },
  {
   "source": [
    "# Gradients"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.backward() is equivalent to doing out.backward(torch.tensor([1.0]))\n",
    "# backprop\n",
    "a.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-1.5000,  0.0000],\n        [ 1.5000,  3.0000]])\n"
     ]
    }
   ],
   "source": [
    "#printing gradients\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([-267.7392,  852.7711, 1088.4725], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Dynamic Graphs\n",
    "x = torch.randn(3, requires_grad =True)\n",
    "\n",
    "y = x * 2\n",
    "i = 0\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    i += 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
     ]
    }
   ],
   "source": [
    "# if we don't run backward on a scalar we need to specify the grad_output\n",
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "source": [
    "## Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this decides the tensor's range below\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1., 1., 1.])\ntensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# both x and w that allows gradient accumulation\n",
    "x = torch.arange(1., n+1, requires_grad = True)\n",
    "w = torch.ones(n, requires_grad = True)\n",
    "z = w @ x \n",
    "z.backward()\n",
    "print(x.grad, w.grad, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\ntensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# only w that allows gradient accumulation\n",
    "x = torch.arange(1., n+1)\n",
    "w = torch.ones(n, requires_grad = True)\n",
    "z = w @ x \n",
    "z.backward()\n",
    "print(x.grad, w.grad, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RuntimeError!!!\nelement 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1., n + 1)\n",
    "w = torch.ones(n, requires_grad = True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = w @ x \n",
    "\n",
    "try:\n",
    "    z.backward()\n",
    "except RuntimeError as e:\n",
    "    print('RuntimeError!!!')\n",
    "    print(e)"
   ]
  }
 ]
}